# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LHqKXcI0a_GUY8s22kDHTlS_pFyIOA5A
"""

import streamlit as st
import numpy as np
import faiss
from groq import Groq
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader
from PIL import Image
import io

# ---------------------------
# CONFIG
# ---------------------------

client = Groq(api_key=st.secrets["GROQ_API_KEY"])
model_name = "openai/gpt-oss-120b"

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

# ---------------------------
# CHUNKING
# ---------------------------
def chunk_text(text):
    chunks = []
    for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):
        chunks.append(text[i:i + CHUNK_SIZE])
    return chunks


# ---------------------------
# CREATE VECTOR STORE
# ---------------------------
def create_vectorstore(text):
    chunks = chunk_text(text)
    embeddings = embedding_model.encode(chunks)

    embeddings = np.array(embeddings).astype("float32")

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)

    return index, chunks


# ---------------------------
# RETRIEVE
# ---------------------------
def retrieve(query, index, chunks, top_k=3):
    query_embedding = embedding_model.encode([query]).astype("float32")
    distances, indices = index.search(query_embedding, top_k)
    return [chunks[i] for i in indices[0]]


# ---------------------------
# GROQ GENERATION
# ---------------------------
def generate_answer(query, context_chunks):
    context = "\n\n".join(context_chunks)

    prompt = f"""
    Answer strictly using the context below.

    Context:
    {context}

    Question:
    {query}

    If answer not found, say:
    Answer not found in document.
    """

    completion = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_completion_tokens=1024,
        stream=False,
    )

    return completion.choices[0].message.content


# ---------------------------
# STREAMLIT UI
# ---------------------------

st.title("ðŸš€ Free Groq RAG Chat")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

uploaded_file = st.file_uploader("Upload PDF", type=["pdf"])
query = st.chat_input("Ask your question")

text_data = ""

if uploaded_file:
    reader = PdfReader(uploaded_file)
    for page in reader.pages:
        text_data += page.extract_text()

    st.success("PDF Processed.")

if query and text_data:
    index, chunks = create_vectorstore(text_data)
    context = retrieve(query, index, chunks)
    answer = generate_answer(query, context)

    st.session_state.chat_history.append(("user", query))
    st.session_state.chat_history.append(("assistant", answer))

for role, message in st.session_state.chat_history:
    with st.chat_message(role):
        st.write(message)
