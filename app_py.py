# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LHqKXcI0a_GUY8s22kDHTlS_pFyIOA5A
"""

import streamlit as st
import numpy as np
import faiss
from groq import Groq
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader
from PIL import Image
import pytesseract
import speech_recognition as sr
import tempfile
import os

# ---------------------------
# CONFIG
# ---------------------------

client = Groq(api_key=st.secrets["GROQ_API_KEY"])
model_name = "openai/gpt-oss-120b"

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

CHUNK_SIZE = 800
CHUNK_OVERLAP = 100


# ---------------------------
# CHUNKING
# ---------------------------
def chunk_text(text):
    chunks = []
    for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):
        chunks.append(text[i:i + CHUNK_SIZE])
    return chunks


# ---------------------------
# CREATE VECTOR STORE
# ---------------------------
def create_vectorstore(text):
    chunks = chunk_text(text)
    embeddings = embedding_model.encode(chunks)
    embeddings = np.array(embeddings).astype("float32")

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)

    return index, chunks


# ---------------------------
# RETRIEVE
# ---------------------------
def retrieve(query, index, chunks, top_k=3):
    query_embedding = embedding_model.encode([query]).astype("float32")
    distances, indices = index.search(query_embedding, top_k)
    return [chunks[i] for i in indices[0]]


# ---------------------------
# GROQ GENERATION
# ---------------------------
def generate_answer(query, context_chunks):
    context = "\n\n".join(context_chunks)

    prompt = f"""
    Answer strictly using the context below.

    Context:
    {context}

    Question:
    {query}

    If answer not found, say:
    Answer not found in document.
    """

    completion = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_completion_tokens=1024,
    )

    return completion.choices[0].message.content


# ---------------------------
# STREAMLIT UI
# ---------------------------

st.title("ðŸš€ Multimodal Groq RAG Assistant")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

text_data = ""

# -------- FILE UPLOAD --------
uploaded_file = st.file_uploader(
    "Upload PDF or Image",
    type=["pdf", "png", "jpg", "jpeg"]
)

if uploaded_file:

    # PDF
    if uploaded_file.type == "application/pdf":
        reader = PdfReader(uploaded_file)
        for page in reader.pages:
            text_data += page.extract_text()

    # Image (OCR)
    else:
        image = Image.open(uploaded_file)
        text_data = pytesseract.image_to_string(image)

    st.success("File processed successfully.")


# -------- VOICE INPUT --------
st.write("### ðŸŽ¤ Voice Input")
audio_file = st.file_uploader("Upload audio (wav format)", type=["wav"])

voice_text = ""
if audio_file:
    recognizer = sr.Recognizer()

    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(audio_file.read())
        tmp_path = tmp_file.name

    with sr.AudioFile(tmp_path) as source:
        audio = recognizer.record(source)
        try:
            voice_text = recognizer.recognize_google(audio)
            st.write("You said:", voice_text)
        except:
            st.error("Could not recognize audio.")

    os.remove(tmp_path)


# -------- TEXT INPUT --------
text_query = st.chat_input("Ask your question")

# Decide which query to use
query = voice_text if voice_text else text_query


# -------- MAIN RAG LOGIC --------
if query and text_data:

    index, chunks = create_vectorstore(text_data)
    context = retrieve(query, index, chunks)
    answer = generate_answer(query, context)

    st.session_state.chat_history.append(("user", query))
    st.session_state.chat_history.append(("assistant", answer))


# -------- DISPLAY CHAT --------
for role, message in st.session_state.chat_history:
    with st.chat_message(role):
        st.write(message)
