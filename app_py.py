# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LHqKXcI0a_GUY8s22kDHTlS_pFyIOA5A
"""

import streamlit as st
import google.generativeai as genai
import numpy as np
import faiss
from PyPDF2 import PdfReader

# ---------------------------
# CONFIGURE GEMINI
# ---------------------------

genai.configure(api_key=st.secrets["GEMINI_API_KEY"])

CHAT_MODEL = "gemini-1.5-flash-latest"
EMBED_MODEL = "embedding-001"

model = genai.GenerativeModel(CHAT_MODEL)

CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

# ---------------------------
# TEXT CHUNKING
# ---------------------------
def chunk_text(text):
    chunks = []
    for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):
        chunks.append(text[i:i + CHUNK_SIZE])
    return chunks


# ---------------------------
# GEMINI EMBEDDING
# ---------------------------
def get_embedding(text):
    response = genai.embed_content(
        model=EMBED_MODEL,
        content=text
    )
    return response["embedding"]


# ---------------------------
# CREATE VECTOR STORE
# ---------------------------
def create_vectorstore(text):
    chunks = chunk_text(text)

    embeddings = [get_embedding(chunk) for chunk in chunks]
    embeddings = np.array(embeddings).astype("float32")

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)

    return index, chunks


# ---------------------------
# RETRIEVE CONTEXT
# ---------------------------
def retrieve(query, index, chunks, top_k=3):
    query_embedding = np.array([get_embedding(query)]).astype("float32")
    distances, indices = index.search(query_embedding, top_k)
    return [chunks[i] for i in indices[0]]


# ---------------------------
# STREAMLIT UI
# ---------------------------

st.title("ðŸ”® Gemini Multimodal RAG Chat")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

uploaded_file = st.file_uploader("Upload PDF or Image", type=["pdf", "png", "jpg", "jpeg"])
query = st.chat_input("Ask your question")

text_data = ""

# ---------------------------
# FILE PROCESSING
# ---------------------------

if uploaded_file:

    if uploaded_file.type == "application/pdf":
        reader = PdfReader(uploaded_file)
        for page in reader.pages:
            text_data += page.extract_text()

    else:
        image_bytes = uploaded_file.read()
        response = model.generate_content([
            "Extract all readable text from this image.",
            {"mime_type": uploaded_file.type, "data": image_bytes}
        ])
        text_data = response.text

    st.success("File processed successfully.")


# ---------------------------
# MAIN CHAT LOGIC
# ---------------------------

if query and text_data:

    index, chunks = create_vectorstore(text_data)
    context = retrieve(query, index, chunks)

    prompt = f"""
    Answer strictly using the context below.

    Context:
    {context}

    Question:
    {query}

    If answer not found, say:
    Answer not found in the document.
    """

    response = model.generate_content(prompt)

    st.session_state.chat_history.append(("user", query))
    st.session_state.chat_history.append(("assistant", response.text))


# ---------------------------
# DISPLAY CHAT
# ---------------------------

for role, message in st.session_state.chat_history:
    with st.chat_message(role):
        st.write(message)
