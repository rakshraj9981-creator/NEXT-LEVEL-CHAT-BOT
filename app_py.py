# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LHqKXcI0a_GUY8s22kDHTlS_pFyIOA5A
"""

import streamlit as st
import os
import faiss
import pickle
import numpy as np
from PyPDF2 import PdfReader
from PIL import Image
import pytesseract
import speech_recognition as sr
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

# -------- TEXT CHUNKING --------
def chunk_text(text):
    chunks = []
    for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):
        chunks.append(text[i:i + CHUNK_SIZE])
    return chunks


# -------- EMBEDDING --------
def get_embedding(text):
    response = openai.Embedding.create(
        model="text-embedding-3-small",
        input=text
    )
    return response["data"][0]["embedding"]


# -------- VECTOR STORE --------
def create_vectorstore(text_data):
    chunks = chunk_text(text_data)

    embeddings = []
    for chunk in chunks:
        embeddings.append(get_embedding(chunk))

    embeddings = np.array(embeddings).astype("float32")

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)

    return index, chunks


# -------- RETRIEVAL --------
def retrieve(query, index, chunks, top_k=3):
    query_embedding = np.array([get_embedding(query)]).astype("float32")
    distances, indices = index.search(query_embedding, top_k)
    return [chunks[i] for i in indices[0]]


# -------- LLM --------
def generate_answer(query, context_chunks):
    context = "\n\n".join(context_chunks)

    prompt = f"""
    Answer the question using only the context below.

    Context:
    {context}

    Question:
    {query}

    If answer not in context, say:
    "Answer not found in provided data."
    """

    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return response["choices"][0]["message"]["content"]


# -------- STREAMLIT UI --------

st.title("ðŸ”® Multimodal RAG Chat Assistant")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

uploaded_file = st.file_uploader("Upload PDF or Image", type=["pdf", "png", "jpg", "jpeg"])
voice_input = st.checkbox("Use Voice Input")

text_data = ""

# -------- FILE PROCESSING --------
if uploaded_file:
    if uploaded_file.type == "application/pdf":
        reader = PdfReader(uploaded_file)
        for page in reader.pages:
            text_data += page.extract_text()

    else:
        image = Image.open(uploaded_file)
        text_data = pytesseract.image_to_string(image)

    st.success("File processed successfully.")

# -------- VOICE INPUT --------
if voice_input:
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.info("Speak now...")
        audio = recognizer.listen(source)
        try:
            query = recognizer.recognize_google(audio)
            st.write("You said:", query)
        except:
            query = ""
            st.error("Could not recognize speech.")
else:
    query = st.chat_input("Ask your question")

# -------- MAIN CHAT LOGIC --------
if query and text_data:

    index, chunks = create_vectorstore(text_data)
    context = retrieve(query, index, chunks)
    answer = generate_answer(query, context)

    st.session_state.chat_history.append(("user", query))
    st.session_state.chat_history.append(("assistant", answer))

# -------- CHAT DISPLAY --------
for role, message in st.session_state.chat_history:
    with st.chat_message(role):
        st.write(message)